{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "logs_path = \"/home/ape/Jupyter/logs\"\n",
    "vocab_file = \"aclImdb/imdb.vocab\"\n",
    "train_file = \"aclImdb/train/labeledBow.feat\"\n",
    "test_file = \"aclImdb/test/labeledBow.feat\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 25000\n",
      "test size: 1000\n",
      "Vocab size: 89527\n"
     ]
    }
   ],
   "source": [
    "# text utils\n",
    "train_lines = []\n",
    "train_lines = open(train_file, \"r\").readlines()\n",
    "random.shuffle(train_lines)\n",
    "print(\"train size:\", len(train_lines))\n",
    "\n",
    "test_lines = []\n",
    "test_lines = open(test_file, \"r\").readlines()\n",
    "random.shuffle(test_lines)\n",
    "test_lines = test_lines[:1000]\n",
    "print(\"test size:\", len(test_lines))\n",
    "\n",
    "vocab = open(vocab_file, \"r\").readlines()\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\",vocab_size)\n",
    "\n",
    "current_line = 0\n",
    "\n",
    "def get_batch(lines, batch_size):\n",
    "    global current_line\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        x = np.zeros(vocab_size, np.int32)\n",
    "        line = [int(s) for s in re.split(\"[ :\\n]\",lines[current_line]) if s.isdigit()]\n",
    "        current_line += 1\n",
    "        \n",
    "        for k in range(1,len(line)-1,2):\n",
    "            x[line[k]] = int(line[k+1])\n",
    "            \n",
    "        batch_x.append(x) # doc2vec\n",
    "        batch_y.append([0] if line[0]>5 else [1])\n",
    "\n",
    "    return [np.asarray(batch_x), np.asarray(batch_y)]\n",
    "\n",
    "# example\n",
    "# x, y = get_batch(train_lines, 5)\n",
    "# print(\"batch x format:\\n\", x)\n",
    "# print(\"batch y format:\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_labels = 1\n",
    "\n",
    "# with tf.name_scope('model'):\n",
    "x = tf.placeholder(tf.float32, [None, vocab_size], name = 'x')\n",
    "y = tf.placeholder(tf.int32, [None, num_labels], name = 'y')\n",
    "\n",
    "W = tf.Variable(tf.random_normal([vocab_size, num_labels], mean=0, stddev=0.1), name=\"W\")\n",
    "b = tf.Variable(tf.random_normal([1, num_labels], mean=0, stddev=0.1), name=\"b\")\n",
    "\n",
    "# h = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=h, labels=y))\n",
    "# loss = tf.reduce_mean( -tf.reduce_sum( y*tf.log(h), reduction_indices=1))\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# pred = tf.equal(tf.argmax(h, 1), tf.argmax(y, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(pred, tf.float32))\n",
    "\n",
    "h = tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
    "\n",
    "loss = cost = tf.losses.mean_squared_error(y, h)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "pred = tf.equal(tf.cast(tf.round(h), tf.int32), y)\n",
    "accuracy = tf.reduce_mean(tf.cast(pred, tf.float32))\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Epoch 1\n",
      "step 10 loss: 0.26713\n",
      "step 20 loss: 0.16851\n",
      "step 30 loss: 0.13145\n",
      "step 40 loss: 0.14038\n",
      "step 50 loss: 0.09475\n",
      "step 60 loss: 0.10111\n",
      "step 70 loss: 0.08782\n",
      "step 80 loss: 0.08510\n",
      "step 90 loss: 0.09846\n",
      "step 100 loss: 0.08844\n",
      "Train accuracy: 0.814440011382 average loss: 0.132092470415\n",
      "Test accuracy: 0.866000026464\n",
      "\n",
      "Epoch 2\n",
      "step 10 loss: 0.06284\n",
      "step 20 loss: 0.07092\n",
      "step 30 loss: 0.05778\n",
      "step 40 loss: 0.05822\n",
      "step 50 loss: 0.04830\n",
      "step 60 loss: 0.05041\n",
      "step 70 loss: 0.04252\n",
      "step 80 loss: 0.03865\n",
      "step 90 loss: 0.05494\n",
      "step 100 loss: 0.04366\n",
      "Train accuracy: 0.94340005219 average loss: 0.0490690544061\n",
      "Test accuracy: 0.879999995232\n",
      "\n",
      "Epoch 3\n",
      "step 10 loss: 0.03470\n",
      "step 20 loss: 0.04574\n",
      "step 30 loss: 0.03457\n",
      "step 40 loss: 0.03456\n",
      "step 50 loss: 0.02444\n",
      "step 60 loss: 0.04106\n",
      "step 70 loss: 0.02509\n",
      "step 80 loss: 0.03604\n",
      "step 90 loss: 0.03381\n",
      "step 100 loss: 0.02928\n",
      "Train accuracy: 0.970440072417 average loss: 0.0304984384682\n",
      "Test accuracy: 0.867999970913\n",
      "\n",
      "Epoch 4\n",
      "step 10 loss: 0.02635\n",
      "step 20 loss: 0.02536\n",
      "step 30 loss: 0.02189\n",
      "step 40 loss: 0.01847\n",
      "step 50 loss: 0.01842\n",
      "step 60 loss: 0.02076\n",
      "step 70 loss: 0.02912\n",
      "step 80 loss: 0.02372\n",
      "step 90 loss: 0.03849\n",
      "step 100 loss: 0.02796\n",
      "Train accuracy: 0.978840077519 average loss: 0.0230798612721\n",
      "Test accuracy: 0.856000006199\n",
      "\n",
      "Epoch 5\n",
      "step 10 loss: 0.01632\n",
      "step 20 loss: 0.01710\n",
      "step 30 loss: 0.01472\n",
      "step 40 loss: 0.02169\n",
      "step 50 loss: 0.01760\n",
      "step 60 loss: 0.01740\n",
      "step 70 loss: 0.01405\n",
      "step 80 loss: 0.01545\n",
      "step 90 loss: 0.03529\n",
      "step 100 loss: 0.01672\n",
      "Train accuracy: 0.985680074096 average loss: 0.0168853349285\n",
      "Test accuracy: 0.863999992609\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 250\n",
    "train_iterations = len(train_lines)//batch_size\n",
    "test_iterations = len(test_lines)//batch_size\n",
    "\n",
    "log_period = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter(logs_path, sess.graph)\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nEpoch: {}/{}\".format(epoch+1, epochs))\n",
    "        current_line = 0\n",
    "        avg_loss = 0.\n",
    "        avg_acc = 0.\n",
    "        \n",
    "        for i in range(train_iterations):\n",
    "            batch_x, batch_y = get_batch(train_lines, batch_size)\n",
    "            \n",
    "            _, l, a, s = sess.run([optimizer, loss, accuracy, merged], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            avg_loss += l / train_iterations\n",
    "            avg_acc += a / train_iterations\n",
    "\n",
    "            writer.add_summary(s, i+epoch*train_iterations)\n",
    "\n",
    "            if (i+1)%10==0:\n",
    "                print(\"step\", i+1, \"loss:\", \"{0:.5f}\".format(l))\n",
    "            \n",
    "        print(\"Train accuracy:\", avg_acc, \"Average loss:\", avg_loss)\n",
    "\n",
    "        current_line = 0\n",
    "        avg_acc = 0.\n",
    "\n",
    "        for i in range(test_iterations):\n",
    "            batch_x, batch_y = get_batch(test_lines, batch_size)\n",
    "            \n",
    "            a = accuracy.eval(feed_dict={x: batch_x, y: batch_y})\n",
    "            avg_acc += a / test_iterations\n",
    "            \n",
    "        print(\"Test accuracy:\", avg_acc)\n",
    "\n",
    "    writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
