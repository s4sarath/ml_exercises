{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from six.moves import xrange\n",
    "\n",
    "vocab_file = \"aclImdb/imdb.vocab\"\n",
    "train_file = \"aclImdb/train/labeledBow.feat\"\n",
    "test_file = \"aclImdb/test/labeledBow.feat\"\n",
    "train_pos_dir = \"aclImdb/train/neg/*.txt\"\n",
    "train_neg_dir = \"aclImdb/train/pos/*.txt\"\n",
    "test_pos_dir = \"aclImdb/test/neg/*.txt\"\n",
    "test_neg_dir = \"aclImdb/test/pos/*.txt\"\n",
    "stopwords_file = \"aclImdb/stopwords.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files number: 25000\n",
      "Test files number: 1000\n",
      "Vocab size: 89527\n"
     ]
    }
   ],
   "source": [
    "train_file_list = glob.glob(train_neg_dir, recursive=True) + glob.glob(train_pos_dir, recursive=True)\n",
    "test_file_list = glob.glob(test_neg_dir, recursive=True) + glob.glob(test_pos_dir, recursive=True)\n",
    "\n",
    "random.shuffle(train_file_list) \n",
    "random.shuffle(test_file_list)\n",
    "print(\"Train files number:\",len(train_file_list))\n",
    "test_file_list = test_file_list[:1000]\n",
    "print(\"Test files number:\",len(test_file_list))\n",
    "\n",
    "vocab = open(vocab_file, \"r\").readlines()\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\",vocab_size)\n",
    "\n",
    "id_to_word = {}\n",
    "for i in range(vocab_size):\n",
    "    id_to_word[i] = vocab[i][:-1]\n",
    "word_to_id = {v: k for k, v in id_to_word.items()}\n",
    "\n",
    "def clean(line):\n",
    "    line = line.replace(\"<br />\",\"\")\n",
    "    line = re.sub('[,.!?]', 'a', line)\n",
    "    line = line.split()\n",
    "    line = [x for x in line]\n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TEXT UTILS\n",
    "current_file = 0\n",
    "\n",
    "def get_batch_for_embedding(file_list):\n",
    "    global current_file\n",
    "    file = file_list[current_file]\n",
    "    current_file += 1\n",
    "    \n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    \n",
    "    lines = open(file, \"r\").readlines()\n",
    "    for line in lines:\n",
    "        line = clean(line)\n",
    "\n",
    "        for i, word in enumerate(line):\n",
    "            if word in word_to_id:\n",
    "                if i != 0 and i < len(line)-1:\n",
    "                    if line[i-1] in word_to_id and line[i+1] in word_to_id:\n",
    "                        batch_x.append(word_to_id[word])\n",
    "                        batch_x.append(word_to_id[word])\n",
    "                        batch_y.append(word_to_id[line[i-1]])\n",
    "                        batch_y.append(word_to_id[line[i+1]])\n",
    "    batch_x = np.array(batch_x)#.reshape((len(batch_x), 1))\n",
    "    batch_y = np.array(batch_y).reshape((len(batch_y), 1))\n",
    "    return [batch_x, batch_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54,)\n",
      "(54, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_x, batch_y = get_batch_for_embedding(train_file_list)\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EMBEDDING GRAPH\n",
    "\n",
    "embed_size = 32    # Dimension of the embedding vector\n",
    "num_sampled = 16    # Number of negative examples to sample.\n",
    "emb_learning_rate = 1.0\n",
    "\n",
    "# Validation samples: most frequent words\n",
    "valid_size = 8      # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "emb_graph = tf.Graph()\n",
    "with emb_graph.as_default():\n",
    "\n",
    "    # input\n",
    "    emb_x = tf.placeholder(tf.int32, shape=[None])\n",
    "    emb_y = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # embeddings\n",
    "    embeddings = tf.Variable( tf.random_uniform( [vocab_size, embed_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup( embeddings, emb_x)\n",
    "\n",
    "    # variables for the NCE loss\n",
    "    emb_W = tf.Variable( tf.truncated_normal( [vocab_size, embed_size], stddev=1.0/math.sqrt(embed_size)))\n",
    "    emb_b = tf.Variable( tf.zeros( [vocab_size]))\n",
    "    \n",
    "    # trainning\n",
    "    # avg NCE loss for a batch (automatically draws a new sample of the neg labels each time we eval the loss)\n",
    "    emb_loss = tf.reduce_mean( tf.nn.nce_loss(weights=emb_W, biases=emb_b, labels=emb_y, inputs=embed, num_sampled=num_sampled, num_classes=vocab_size))\n",
    "    emb_optimizer = tf.train.GradientDescentOptimizer(emb_learning_rate).minimize(emb_loss)\n",
    "\n",
    "    # similarity\n",
    "    norm = tf.sqrt( tf.reduce_sum( tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup( normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul( valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING EMBEDDINGS\n",
      "\n",
      "Initialized\n",
      "Avg loss at step 1000 : 66.8660494633\n",
      "Avg loss at step 2000 : 51.5613375311\n",
      "Avg loss at step 3000 : 44.1701982212\n",
      "Avg loss at step 4000 : 40.1181295128\n",
      "Avg loss at step 5000 : 36.341986237\n",
      "\n",
      "Step 5000 similarity eval:\n",
      "Nearest to been : must thin overall started seen familiar \n",
      "Nearest to than : feature feeling action exceptional fun forced \n",
      "Nearest to if : will why wanted kill told unique \n",
      "Nearest to them : out car once ita role exciting \n",
      "Nearest to all : turns final probably plot familiar tried \n",
      "Nearest to don't : can thema would leave loved preview \n",
      "Nearest to ? : kadal accomplish observably theoretically non-actors defensive \n",
      "Nearest to me : won indie need throws turns crap \n",
      "\n",
      "Avg loss at step 6000 : 34.2281350021\n",
      "Avg loss at step 7000 : 32.0809573855\n",
      "Avg loss at step 8000 : 29.8505212302\n",
      "Avg loss at step 9000 : 28.3975401983\n",
      "Avg loss at step 10000 : 26.2993593471\n",
      "\n",
      "Step 10000 similarity eval:\n",
      "Nearest to been : must thin hang seen overall familiar \n",
      "Nearest to than : exceptional bucks demanding during are feature \n",
      "Nearest to if : why unique performer will told wanted \n",
      "Nearest to them : something ita car out fans exciting \n",
      "Nearest to all : turns rated probably performances each commercial \n",
      "Nearest to don't : can preview doesn't would thema spooky \n",
      "Nearest to ? : kadal accomplish observably theoretically non-actors defensive \n",
      "Nearest to me : throws indie us won crap need \n",
      "\n",
      "Avg loss at step 11000 : 25.043869849\n",
      "Avg loss at step 12000 : 25.0204123721\n",
      "Avg loss at step 13000 : 23.3200195181\n",
      "Avg loss at step 14000 : 22.1304044592\n",
      "Avg loss at step 15000 : 21.4439430015\n",
      "\n",
      "Step 15000 similarity eval:\n",
      "Nearest to been : must thin hang seen six nightmare \n",
      "Nearest to than : bucks exceptional demanding during correctness student \n",
      "Nearest to if : why nod unique ended when performer \n",
      "Nearest to them : ita something us plan fans him \n",
      "Nearest to all : turns strictly episodes warning incredible dealing \n",
      "Nearest to don't : can doesn't would spooky preview didn't \n",
      "Nearest to ? : kadal observably defensive non-actors theoretically accomplish \n",
      "Nearest to me : us throws indie timeless won described \n",
      "\n",
      "Avg loss at step 16000 : 19.9876733007\n",
      "Avg loss at step 17000 : 19.9542950675\n",
      "Avg loss at step 18000 : 19.3384890072\n",
      "Avg loss at step 19000 : 18.4722382808\n",
      "Avg loss at step 20000 : 17.4161066628\n",
      "\n",
      "Step 20000 similarity eval:\n",
      "Nearest to been : thin hang six overrate roll must \n",
      "Nearest to than : bucks during student companion exceptional demanding \n",
      "Nearest to if : why when nod ended unique antics \n",
      "Nearest to them : him us something ita her fans \n",
      "Nearest to all : strictly episodes warning incredible turns dealing \n",
      "Nearest to don't : can doesn't didn't spooky exactly contradictive \n",
      "Nearest to ? : kadal observably defensive non-actors theoretically grabbers \n",
      "Nearest to me : us throws indie him timeless inspirational \n",
      "\n",
      "Avg loss at step 21000 : 17.8345143604\n",
      "Avg loss at step 22000 : 17.3614500499\n",
      "Avg loss at step 23000 : 16.4378815901\n",
      "Avg loss at step 24000 : 15.5904364417\n",
      "Avg loss at step 25000 : 15.5464711642\n",
      "\n",
      "Step 25000 similarity eval:\n",
      "Nearest to been : thin six hang glued nightmare roll \n",
      "Nearest to than : bucks shea student convict exceptional daily \n",
      "Nearest to if : when why nod ended antics unique \n",
      "Nearest to them : him us something ita anything her \n",
      "Nearest to all : episodes strictly guys previous lying warning \n",
      "Nearest to don't : doesn't can didn't can't won't exactly \n",
      "Nearest to ? : kadal observably defensive theoretically grabbers chamas \n",
      "Nearest to me : us throws him timeless indie inspirational \n",
      "\n",
      "Avg loss at step 1000 : 14.7837330339\n",
      "Avg loss at step 2000 : 14.7364681945\n",
      "Avg loss at step 3000 : 13.9402518651\n",
      "Avg loss at step 4000 : 13.8985244677\n",
      "Avg loss at step 5000 : 13.2295421438\n",
      "\n",
      "Step 5000 similarity eval:\n",
      "Nearest to been : thin six hang glued roll facsimile \n",
      "Nearest to than : student convict bucks shea exceptional salvation \n",
      "Nearest to if : when why antics owns forest nod \n",
      "Nearest to them : him us something ita yourself plan \n",
      "Nearest to all : episodes warning guys strictly dealing raised \n",
      "Nearest to don't : can doesn't didn't can't won't spooky \n",
      "Nearest to ? : kadal observably defensive theoretically grabbers chamas \n",
      "Nearest to me : us him throws dedicated forum inspirational \n",
      "\n",
      "Avg loss at step 6000 : 12.7950595856\n",
      "Avg loss at step 7000 : 13.0602118537\n",
      "Avg loss at step 8000 : 13.154294878\n",
      "Avg loss at step 9000 : 12.2963480048\n",
      "Avg loss at step 10000 : 11.8052008085\n",
      "\n",
      "Step 10000 similarity eval:\n",
      "Nearest to been : thin six glued hang expertise got \n",
      "Nearest to than : student convict salvation bucks shea daily \n",
      "Nearest to if : when why owns unnecessarily wore though \n",
      "Nearest to them : him us something yourself ita lying \n",
      "Nearest to all : hole previous dealing episodes thank warning \n",
      "Nearest to don't : can doesn't can't didn't won't wouldn't \n",
      "Nearest to ? : kadal observably defensive theoretically grabbers chamas \n",
      "Nearest to me : us him forum timeless dedicated throws \n",
      "\n",
      "Avg loss at step 11000 : 12.1002794983\n",
      "Avg loss at step 12000 : 12.0073055649\n",
      "Avg loss at step 13000 : 11.2374821234\n",
      "Avg loss at step 14000 : 11.1012263668\n",
      "Avg loss at step 15000 : 10.7693486209\n",
      "\n",
      "Step 15000 similarity eval:\n",
      "Nearest to been : six thin overrate facsimile townsfolk expertise \n",
      "Nearest to than : student shea salvation convict daily enchanting \n",
      "Nearest to if : when owns wore unnecessarily though why \n",
      "Nearest to them : him us yourself ita lying something \n",
      "Nearest to all : episodes dealing hole guys thank previous \n",
      "Nearest to don't : didn't can't can doesn't won't wouldn't \n",
      "Nearest to ? : kadal observably defensive grabbers chamas falworth \n",
      "Nearest to me : us him forum timeless dedicated throws \n",
      "\n",
      "Avg loss at step 16000 : 10.2897538607\n",
      "Avg loss at step 17000 : 10.3725083351\n",
      "Avg loss at step 18000 : 9.85234839368\n",
      "Avg loss at step 19000 : 9.83794342852\n",
      "Avg loss at step 20000 : 9.65547577286\n",
      "\n",
      "Step 20000 similarity eval:\n",
      "Nearest to been : six thin expertise facsimile hang townsfolk \n",
      "Nearest to than : student salvation uniquely daily enchanting or \n",
      "Nearest to if : when owns though causal wore unnecessarily \n",
      "Nearest to them : him us yourself ita myself lying \n",
      "Nearest to all : hole guys raised episodes ratings thank \n",
      "Nearest to don't : didn't can't doesn't won't can wouldn't \n",
      "Nearest to ? : kadal observably defensive grabbers chamas rebuild \n",
      "Nearest to me : us him forum timeless them dedicated \n",
      "\n",
      "Avg loss at step 21000 : 9.55627872658\n",
      "Avg loss at step 22000 : 9.65224158311\n",
      "Avg loss at step 23000 : 9.60827982211\n",
      "Avg loss at step 24000 : 8.89480698586\n",
      "Avg loss at step 25000 : 8.80485859394\n",
      "\n",
      "Step 25000 similarity eval:\n",
      "Nearest to been : thin six facsimile expertise townsfolk hang \n",
      "Nearest to than : student salvation uniquely or enchanting daily \n",
      "Nearest to if : when owns though wore causal unless \n",
      "Nearest to them : him us yourself ita myself something \n",
      "Nearest to all : hole guys episodes raised outbreak fifty \n",
      "Nearest to don't : didn't can't won't doesn't can wouldn't \n",
      "Nearest to ? : kadal observably defensive grabbers chamas rebuild \n",
      "Nearest to me : us him them forum timeless icons \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTRAINING EMBEDDINGS\\n\")\n",
    "training_epochs = 2\n",
    "num_steps = 25000\n",
    "print_step = 1000\n",
    "\n",
    "with tf.Session(graph=emb_graph) as session:\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        avg_loss = 0\n",
    "        current_file = 0\n",
    "\n",
    "        for step in xrange(num_steps):\n",
    "            batch_x, batch_y = get_batch_for_embedding(train_file_list)\n",
    "            feed_dict = {emb_x: batch_x, emb_y: batch_y}\n",
    "\n",
    "            _, loss = session.run([emb_optimizer, emb_loss], feed_dict=feed_dict)\n",
    "            if not math.isnan(loss/print_step):\n",
    "                avg_loss += loss/print_step\n",
    "\n",
    "            if (step+1) % print_step == 0:\n",
    "                print('Avg loss at step', step+1, ':', avg_loss)\n",
    "                avg_loss = 0\n",
    "\n",
    "            if (step+1) % 5000 == 0:\n",
    "                print(\"\\nStep\", step+1, \"similarity eval:\")\n",
    "                sim = similarity.eval()\n",
    "                for i in xrange(valid_size):\n",
    "                    valid_word = id_to_word[valid_examples[i]]\n",
    "                    top_k = 6  # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                    log_str=\"\"\n",
    "                    for k in xrange(top_k):\n",
    "                        log_str += id_to_word[nearest[k]]+\" \"\n",
    "                    print(\"Nearest to\", valid_word, \":\", log_str)\n",
    "                print(\"\")\n",
    "\n",
    "    final_embeddings = normalized_embeddings.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VISUALIZE EMBEDDINGS\n",
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_with_labels(low_dim_embs, labels, filename='embed.png'):\n",
    "#     assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "#     plt.figure(figsize=(18, 18))    # in inches\n",
    "#     for i, label in enumerate(labels):\n",
    "#         x, y = low_dim_embs[i, :]\n",
    "#         plt.scatter(x, y)\n",
    "#         plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "#     plt.savefig(filename)\n",
    "\n",
    "# tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "# plot_only = 500\n",
    "# low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "# labels = [id_to_word[i] for i in xrange(plot_only)]\n",
    "# plot_with_labels(low_dim_embs, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT UTILS\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "words_per_review = 50\n",
    "batch_size = 250\n",
    "current_file = 0\n",
    "\n",
    "def get_tokenized(file_list):\n",
    "    global current_file\n",
    "    file = file_list[current_file]\n",
    "    current_file += 1\n",
    "    \n",
    "    batch_x = []\n",
    "    lines = open(file, \"r\").readlines()\n",
    "    for line in lines:\n",
    "        for i, word in enumerate(clean(line)):\n",
    "            if word in word_to_id and word not in stopwords:\n",
    "                batch_x.append(word_to_id[word])\n",
    "    \n",
    "    batch_x = np.array(pad_sequences([batch_x], maxlen = words_per_review)).reshape(words_per_review)\n",
    "    \n",
    "    file = [s for s in re.split(\"[._]\",file)]\n",
    "    val_y = int(file[-2])\n",
    "    batch_y = [0] if val_y>5 else [1]\n",
    "    batch_y = np.array(batch_y)\n",
    "    return [batch_x, batch_y]\n",
    "\n",
    "def get_batch_for_classification(file_list):\n",
    "    x = []\n",
    "    y = []\n",
    "    for step in range(batch_size):\n",
    "        batch_x, batch_y = get_tokenized(file_list)\n",
    "        x.append(batch_x)\n",
    "        y.append(batch_y)\n",
    "    return [np.array(x), np.array(y)]\n",
    "\n",
    "# example\n",
    "# batch_x, batch_y = get_batch_for_classification(train_file_list)\n",
    "# print(\"batch x format:\", batch_x.shape)\n",
    "# print(\"batch y format:\", batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFICATION GRAPH\n",
    "cls_learning_rate = 0.01\n",
    "num_labels = 1\n",
    "\n",
    "cls_graph = tf.Graph()\n",
    "with cls_graph.as_default():\n",
    "   \n",
    "    # input \n",
    "    cls_x = tf.placeholder(tf.int32, [None, words_per_review])\n",
    "    cls_y = tf.placeholder(tf.float32, [None, num_labels])\n",
    "\n",
    "    embed = tf.nn.embedding_lookup(final_embeddings, cls_x)\n",
    "    embed_avg = tf.reduce_mean(embed, 1)\n",
    "    \n",
    "    # variables\n",
    "    cls_W = tf.Variable(tf.random_normal([embed_size, num_labels], mean=0, stddev=0.1))\n",
    "    cls_b = tf.Variable(tf.random_normal([num_labels], mean=0, stddev=0.1))\n",
    "\n",
    "    cls_h = tf.nn.sigmoid(tf.matmul(embed_avg,cls_W) + cls_b)\n",
    "\n",
    "    cls_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=cls_h, labels=cls_y))\n",
    "    cls_optimizer = tf.train.GradientDescentOptimizer(cls_learning_rate).minimize(cls_loss)\n",
    "\n",
    "#     cls_h = tf.nn.softmax(tf.matmul(embed_avg,cls_W) + cls_b)\n",
    "#     cls_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=cls_h, labels=cls_y))\n",
    "#     cls_loss = tf.reduce_mean(-tf.reduce_sum( cls_y*tf.log(cls_h), reduction_indices=1))\n",
    "#     cls_pred = tf.equal(tf.argmax(cls_h, 1), tf.argmax(cls_y, 1))\n",
    "\n",
    "    cls_pred = tf.equal(tf.round(cls_h), cls_y)\n",
    "    cls_accuracy = tf.reduce_mean(tf.cast(cls_pred, tf.float32))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING CLASSIFICATION\n",
      "\n",
      "Initialized\n",
      "\n",
      "Epoch: 1/5\n",
      "step 10 loss: 0.70440\n",
      "step 20 loss: 0.69986\n",
      "step 30 loss: 0.73716\n",
      "step 40 loss: 0.72376\n",
      "step 50 loss: 0.71480\n",
      "step 60 loss: 0.70594\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-882d8d7eb0b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch_for_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#             print(batch_x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcls_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_accuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcls_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-046219eb7c89>\u001b[0m in \u001b[0;36mget_batch_for_classification\u001b[0;34m(file_list)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-046219eb7c89>\u001b[0m in \u001b[0;36mget_tokenized\u001b[0;34m(file_list)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m# and word not in stopwords:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-4f6f2dd450cc>\u001b[0m in \u001b[0;36mclean\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<br />\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[,.!?]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"\\nTRAINING CLASSIFICATION\\n\")\n",
    "epochs = 5\n",
    "train_iterations = len(train_file_list)//batch_size\n",
    "test_iterations = len(test_file_list)//batch_size\n",
    "\n",
    "with tf.Session(graph = cls_graph) as sess:\n",
    "    sess.run(init)\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nEpoch: {}/{}\".format(epoch+1, epochs))\n",
    "        \n",
    "        current_file = 0\n",
    "        avg_loss = 0.\n",
    "        avg_acc = 0.\n",
    "        \n",
    "        for i in range(train_iterations):\n",
    "            batch_x, batch_y = get_batch_for_classification(train_file_list)\n",
    "#             print(batch_x)\n",
    "            _,l, a = sess.run([cls_optimizer,cls_loss, cls_accuracy], feed_dict={cls_x: batch_x, cls_y: batch_y})\n",
    "            \n",
    "            avg_loss += l/train_iterations if not math.isnan(l) else 0\n",
    "            avg_acc += a / train_iterations\n",
    "\n",
    "            if (i+1)%10==0:\n",
    "                print(\"step\", i+1, \"loss:\", \"{0:.5f}\".format(l))\n",
    "                    \n",
    "        print(\"Train accuracy:\", avg_acc, \"Average loss:\", avg_loss)\n",
    "        \n",
    "        current_file = 0\n",
    "        avg_acc = 0.\n",
    "        \n",
    "        for i in range(test_iterations):\n",
    "            batch_x, batch_y = get_batch_for_classification(test_file_list)\n",
    "            \n",
    "            a = cls_accuracy.eval(feed_dict={cls_x: batch_x, cls_y: batch_y})\n",
    "            avg_acc += a/test_iterations if not math.isnan(a) else 0\n",
    "            \n",
    "        print(\"Test accuracy:\", avg_acc, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04803156  0.30272895  0.30717245 -0.0146778   0.06636044 -0.05380407\n",
      " -0.09432425 -0.07076189  0.07958436 -0.12901697 -0.0738842  -0.35219225\n",
      "  0.4181647  -0.15704927 -0.2716797  -0.07507171 -0.31720325  0.14018102\n",
      " -0.10465492  0.02995977  0.14486913  0.04291577  0.35885289  0.11590518\n",
      "  0.00678463  0.06071221 -0.1491105  -0.08209922  0.05052676  0.0713219\n",
      "  0.0322271  -0.14242215]\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "print(final_embeddings[0])\n",
    "print(id_to_word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
