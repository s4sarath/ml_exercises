{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from six.moves import xrange\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_file = \"aclImdb/imdb.vocab\"\n",
    "train_file = \"aclImdb/train/labeledBow.feat\"\n",
    "test_file = \"aclImdb/test/labeledBow.feat\"\n",
    "train_pos_dir = \"aclImdb/train/neg/*.txt\"\n",
    "train_neg_dir = \"aclImdb/train/pos/*.txt\"\n",
    "test_pos_dir = \"aclImdb/test/neg/*.txt\"\n",
    "test_neg_dir = \"aclImdb/test/pos/*.txt\"\n",
    "stopwords_file = \"aclImdb/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files number: 25000\n",
      "Test files number: 1000\n",
      "Vocab size: 89302\n"
     ]
    }
   ],
   "source": [
    "train_file_list = glob.glob(train_neg_dir, recursive=True) + glob.glob(train_pos_dir, recursive=True)\n",
    "test_file_list = glob.glob(test_neg_dir, recursive=True) + glob.glob(test_pos_dir, recursive=True)\n",
    "\n",
    "random.shuffle(train_file_list) \n",
    "random.shuffle(test_file_list)\n",
    "\n",
    "print(\"Train files number:\",len(train_file_list))\n",
    "test_file_list = test_file_list[:1000]\n",
    "print(\"Test files number:\",len(test_file_list))\n",
    "\n",
    "vocab = open(vocab_file, \"r\").readlines()\n",
    "stopwords = open(stopwords_file, \"r\").read()\n",
    "vocab = [x for x in vocab if x not in stopwords]\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\",vocab_size)\n",
    "    \n",
    "id_to_word = {}\n",
    "for i in range(vocab_size):\n",
    "    id_to_word[i] = vocab[i][:-1]\n",
    "word_to_id = {v: k for k, v in id_to_word.items()}\n",
    "\n",
    "def clean(line):\n",
    "    line = line.replace(\"<br />\",\"\")\n",
    "    line = re.sub('[,.!?]', 'a', line)\n",
    "    line = line.split()\n",
    "    line = [x for x in line]\n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT UTILS\n",
    "current_file = 0\n",
    "words_per_review = 50\n",
    "\n",
    "def get_tokenized(file_list):\n",
    "    global current_file\n",
    "    file = file_list[current_file]\n",
    "    current_file += 1\n",
    "    \n",
    "    batch_x = []\n",
    "    line = [s for s in re.split(\"[._]\",file)]\n",
    "    val_y = int(line[-2])\n",
    "#     batch_y = [0, 1] if val_y>5 else [1, 0]\n",
    "    batch_y = [0] if val_y>5 else [1]\n",
    "\n",
    "    lines = open(file, \"r\").readlines()\n",
    "    for line in lines:\n",
    "        line = clean(line)\n",
    "        \n",
    "        for i, word in enumerate(line):\n",
    "            if word in word_to_id:\n",
    "                batch_x.append(word_to_id[word])\n",
    "                        \n",
    "#     batch_x = np.array(batch_x)\n",
    "    batch_x = np.array(pad_sequences([batch_x], maxlen = words_per_review)).reshape(words_per_review)\n",
    "    batch_y = np.array(batch_y)\n",
    "    return [batch_x, batch_y]\n",
    "\n",
    "\n",
    "def get_batch(file_list, batch_size):\n",
    "    x = []\n",
    "    y = []\n",
    "    for step in range(batch_size):\n",
    "        batch_x, batch_y = get_tokenized(file_list)\n",
    "        x.append(batch_x)\n",
    "        y.append(batch_y)\n",
    "    return [x,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n",
      "(1,)\n",
      "[    0     0     0     0   254     1    33    61    40     1   723    92\n",
      "  6032   254    14     1   106     8 29171   734   774   543   236   969\n",
      "  2690   723  1802 21525    80     7  2391  2411  1715  2030 11088  1198\n",
      "  1777   519     9   481   119   119  1257  2015 84790   400     1  8996\n",
      " 70606   882]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "batch_x, batch_y = get_tokenized(train_file_list)\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "print(batch_x)\n",
    "print(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell(lstm_size, keep_prob):\n",
    "    return tf.contrib.rnn.DropoutWrapper( tf.contrib.rnn.BasicLSTMCell(lstm_size), output_keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "def build_rnn(n_words, embed_size, batch_size, lstm_size, num_layers, dropout, learning_rate, multiple_fc, fc_units):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # placeholders\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "\n",
    "    with tf.name_scope('labels'):\n",
    "        labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # embeddings\n",
    "    with tf.name_scope(\"embeddings\"):\n",
    "        embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "\n",
    "\n",
    "    # rnn layers\n",
    "    with tf.name_scope(\"RNN_layers\"):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "        \n",
    "    # Set the initial state\n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)    \n",
    "    \n",
    "    # Create the fully connected layers\n",
    "    with tf.name_scope(\"fully_connected\"):\n",
    "        \n",
    "        # Initialize the weights and biases\n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        \n",
    "        dense = tf.contrib.layers.fully_connected(outputs[:, -1],\n",
    "                                                  num_outputs = fc_units,\n",
    "                                                  activation_fn = tf.sigmoid,\n",
    "                                                  weights_initializer = weights,\n",
    "                                                  biases_initializer = biases)\n",
    "        dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
    "\n",
    "    \n",
    "    with tf.name_scope('predictions'):\n",
    "        predictions = tf.contrib.layers.fully_connected(dense, \n",
    "                                                        num_outputs = 1, \n",
    "                                                        activation_fn=tf.sigmoid,\n",
    "                                                        weights_initializer = weights,\n",
    "                                                        biases_initializer = biases)\n",
    "#         tf.summary.histogram('predictions', predictions)\n",
    "\n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.losses.mean_squared_error(labels, predictions)\n",
    "#         tf.summary.scalar('cost', cost)\n",
    "        \n",
    "\n",
    "    # Train the model\n",
    "    with tf.name_scope('train'):    \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # Determine the accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Merge all of the summaries\n",
    "#     merged = tf.summary.merge_all()    \n",
    "\n",
    "    # Export the nodes\n",
    "    export_nodes = ['inputs', 'labels', 'keep_prob', 'initial_state', 'final_state','accuracy',\n",
    "                    'predictions', 'cost', 'optimizer']#, 'merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(vocab)\n",
    "embed_size = 300\n",
    "batch_size = 250\n",
    "lstm_size = 128\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "learning_rate = 0.001\n",
    "multiple_fc = False\n",
    "fc_units = 256\n",
    "\n",
    "model = build_rnn(n_words = n_words, \n",
    "                  embed_size = embed_size,\n",
    "                  batch_size = batch_size,\n",
    "                  lstm_size = lstm_size,\n",
    "                  num_layers = num_layers,\n",
    "                  dropout = dropout,\n",
    "                  learning_rate = learning_rate,\n",
    "                  multiple_fc = multiple_fc,\n",
    "                  fc_units = fc_units)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.322898 0.456\n",
      "0.232558 0.64\n",
      "0.227968 0.64\n",
      "0.185928 0.744\n",
      "0.20292 0.736\n",
      "Epoch: 0/5 Train Loss: 0.209 Train Acc: 0.682 Valid Loss: 0.145 Valid Acc: 0.791\n",
      "0.153856 0.78\n",
      "0.140713 0.808\n",
      "0.150389 0.788\n",
      "0.128356 0.82\n",
      "0.123197 0.828\n",
      "Epoch: 1/5 Train Loss: 0.132 Train Acc: 0.818 Valid Loss: 0.139 Valid Acc: 0.814\n",
      "0.133732 0.824\n",
      "0.101578 0.86\n",
      "0.118474 0.84\n",
      "0.0890032 0.88\n",
      "0.0960845 0.872\n",
      "Epoch: 2/5 Train Loss: 0.099 Train Acc: 0.872 Valid Loss: 0.144 Valid Acc: 0.806\n",
      "0.107395 0.856\n",
      "0.0681248 0.908\n",
      "0.0798445 0.904\n",
      "0.0643577 0.916\n",
      "0.061133 0.932\n",
      "Epoch: 3/5 Train Loss: 0.075 Train Acc: 0.906 Valid Loss: 0.151 Valid Acc: 0.810\n",
      "0.0711755 0.912\n",
      "0.052582 0.944\n",
      "0.0704055 0.924\n",
      "0.0714792 0.904\n",
      "0.0429966 0.948\n",
      "Epoch: 4/5 Train Loss: 0.065 Train Acc: 0.921 Valid Loss: 0.153 Valid Acc: 0.813\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "train_iterations = len(train_file_list) // batch_size\n",
    "test_iterations = len(test_file_list) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(model.initial_state)\n",
    "        current_file = 0\n",
    "\n",
    "        # Record progress with each epoch\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        val_acc = []\n",
    "        val_loss = []\n",
    "\n",
    "\n",
    "        for it in range(train_iterations):\n",
    "            x, y = get_batch(train_file_list, batch_size)\n",
    "            \n",
    "            feed = {model.inputs: x,\n",
    "                    model.labels: y,\n",
    "                    model.keep_prob: dropout,\n",
    "                    model.initial_state: state}\n",
    "            loss, acc, state, _ = sess.run([model.cost, \n",
    "                                             model.accuracy, \n",
    "                                             model.final_state, \n",
    "                                             model.optimizer], \n",
    "                                            feed_dict=feed)                \n",
    "\n",
    "            # Record the loss and accuracy of each training batch\n",
    "            train_loss.append(loss)\n",
    "            train_acc.append(acc)\n",
    "            if it%20 == 0:\n",
    "                print(loss,acc)\n",
    "\n",
    "\n",
    "        # Average the training loss and accuracy of each epoch\n",
    "        avg_train_loss = np.mean(train_loss)\n",
    "        avg_train_acc = np.mean(train_acc) \n",
    "\n",
    "        val_state = sess.run(model.initial_state)\n",
    "        current_file = 0\n",
    "        \n",
    "        for it in range(test_iterations):\n",
    "            x, y = get_batch(test_file_list, batch_size)\n",
    "            \n",
    "            feed = {model.inputs: x,\n",
    "                    model.labels: y,\n",
    "                    model.keep_prob: 1,\n",
    "                    model.initial_state: val_state}\n",
    "            batch_loss, batch_acc, val_state = sess.run([model.cost, \n",
    "                                                          model.accuracy, \n",
    "                                                          model.final_state], \n",
    "                                                         feed_dict=feed)\n",
    "\n",
    "            # Record the validation loss and accuracy of each epoch\n",
    "            val_loss.append(batch_loss)\n",
    "            val_acc.append(batch_acc)\n",
    "\n",
    "        # Average the validation loss and accuracy of each epoch\n",
    "        avg_valid_loss = np.mean(val_loss)    \n",
    "        avg_valid_acc = np.mean(val_acc)\n",
    "\n",
    "        # Print the progress of each epoch\n",
    "        print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "              \"Train Loss: {:.3f}\".format(avg_train_loss),\n",
    "              \"Train Acc: {:.3f}\".format(avg_train_acc),\n",
    "              \"Valid Loss: {:.3f}\".format(avg_valid_loss),\n",
    "              \"Valid Acc: {:.3f}\".format(avg_valid_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
