{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n",
      "Most common words: [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201), ('a', 325873), ('to', 316376), ('zero', 264975), ('nine', 250430)]\n",
      "Sample data:\n",
      " [5241, 3083, 12, 6, 195, 2, 3134, 46, 59, 156] \n",
      " ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "# reading data\n",
    "file = open(\"text8\", \"r\") \n",
    "vocabulary = tf.compat.as_str(file.read()).split()\n",
    "print('Data size', len(vocabulary))\n",
    "\n",
    "# most common words\n",
    "count = [['UNK', -1]]\n",
    "count.extend(collections.Counter(vocabulary).most_common(vocabulary_size - 1))\n",
    "\n",
    "# embeddings\n",
    "embed = dict()\n",
    "for word, _ in count:\n",
    "    embed[word] = len(embed)\n",
    "\n",
    "rev_embed = dict(zip(embed.values(), embed.keys()))\n",
    "\n",
    "# generating embedded data\n",
    "data = list()\n",
    "unk = 0\n",
    "for word in vocabulary:\n",
    "    if word in embed:\n",
    "        index = embed[word]\n",
    "    else:\n",
    "        index = 0    # embed['UNK']\n",
    "        unk += 1\n",
    "    data.append(index)\n",
    "count[0][1] = unk\n",
    "\n",
    "# to reduce memory\n",
    "del vocabulary\n",
    "\n",
    "print('Most common words:', count[:10])\n",
    "print('Sample data:\\n', data[:10],'\\n', [rev_embed[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Batch: (input word -> output context)\n",
      "3083 originated -> 5241 anarchism\n",
      "3083 originated -> 12 as\n",
      "12 as -> 6 a\n",
      "12 as -> 3083 originated\n",
      "6 a -> 12 as\n",
      "6 a -> 195 term\n",
      "195 term -> 6 a\n",
      "195 term -> 2 of\n"
     ]
    }
   ],
   "source": [
    "# generate a training batch\n",
    "data_index = 0\n",
    "\n",
    "def generate_batch( batch_size, num_skips):\n",
    "    global data_index\n",
    "    skip_window = num_skips//2\n",
    "    assert batch_size % num_skips == 0\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1    # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    \n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window    # target label at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        \n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2)\n",
    "print(\"Example Batch: (input word -> output context)\")\n",
    "for i in range(len(batch)):\n",
    "    print(batch[i], rev_embed[batch[i]],'->', labels[i, 0], rev_embed[labels[i, 0]])\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Build and train a skip-gram model.\n",
    "\n",
    "batch_size = 128\n",
    "embed_size = 128    # Dimension of the embedding vector.\n",
    "num_skips = 2       # How many times to reuse an input to generate a label.\n",
    "\n",
    "\n",
    "# Validation samples: most frequent words\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable( tf.random_uniform( [vocabulary_size, embed_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup( embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    W = tf.Variable( tf.truncated_normal( [vocabulary_size, embed_size], stddev=1.0/math.sqrt(embed_size)))\n",
    "    b = tf.Variable( tf.zeros( [vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # automatically draws a new sample of the nega labels each time we eval the loss.\n",
    "    loss = tf.reduce_mean( tf.nn.nce_loss(weights=W, biases=b, labels=train_labels, \n",
    "                                          inputs=embed, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.      \n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt( tf.reduce_sum( tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup( normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul( valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  295.685028076\n",
      "Nearest to may: columbo, shatter, tropics, lenat,\n",
      "Nearest to years: luthor, bund, sings, induces,\n",
      "Nearest to was: trademarks, gibson, anglian, genji,\n",
      "Nearest to UNK: affleck, drexel, glove, surgically,\n",
      "Nearest to or: rimet, hengest, swordfish, octavio,\n",
      "Nearest to as: stara, sesotho, stouffer, birthrate,\n",
      "Nearest to a: coordinated, porting, rutger, ishtar,\n",
      "Nearest to for: distraction, zubaydah, oyly, sarris,\n",
      "Nearest to system: remake, lookups, norm, marta,\n",
      "Nearest to its: rcito, oc, jackal, lir,\n",
      "Nearest to these: attainder, catacombs, saturday, unalaska,\n",
      "Nearest to up: defamation, psychopathic, foundry, steward,\n",
      "Nearest to see: bagpipe, prescribe, bobble, blatant,\n",
      "Nearest to six: interpolant, gra, resonant, shiloh,\n",
      "Nearest to while: ken, jura, soap, rancho,\n",
      "Nearest to are: cumberland, bosman, detailing, baskets,\n",
      "\n",
      "Average loss at step  2000 :  113.67349466\n",
      "Average loss at step  4000 :  52.040497231\n",
      "Average loss at step  6000 :  33.3352152419\n",
      "Average loss at step  8000 :  23.5432970353\n",
      "Average loss at step  10000 :  17.9141839459\n",
      "Nearest to may: dyke, common, zero, lie,\n",
      "Nearest to years: architecture, luthor, cl, placed,\n",
      "Nearest to was: is, theistic, and, so,\n",
      "Nearest to UNK: and, vs, reginae, jpg,\n",
      "Nearest to or: and, studd, a, complex,\n",
      "Nearest to as: touchdown, is, and, agave,\n",
      "Nearest to a: the, relativity, aberdeen, UNK,\n",
      "Nearest to for: of, in, and, archie,\n",
      "Nearest to system: norm, ep, exceptions, victoriae,\n",
      "Nearest to its: the, trench, commercially, his,\n",
      "Nearest to these: many, the, amphibians, saturday,\n",
      "Nearest to up: defamation, hit, agave, sands,\n",
      "Nearest to see: tubing, zero, keel, organized,\n",
      "Nearest to six: zero, nine, trench, aberdeen,\n",
      "Nearest to while: soap, hakama, ken, atheistic,\n",
      "Nearest to are: were, boroughs, is, cumberland,\n",
      "\n",
      "Average loss at step  12000 :  14.1080223961\n",
      "Average loss at step  14000 :  11.9320438497\n",
      "Average loss at step  16000 :  10.0864293864\n",
      "Average loss at step  18000 :  8.5553821671\n",
      "Average loss at step  20000 :  8.0254277451\n",
      "Nearest to may: would, smalltalk, dasyprocta, berzelius,\n",
      "Nearest to years: architecture, luthor, agouti, two,\n",
      "Nearest to was: is, were, by, are,\n",
      "Nearest to UNK: dasyprocta, agouti, three, circ,\n",
      "Nearest to or: and, dasyprocta, the, studd,\n",
      "Nearest to as: and, by, dasyprocta, touchdown,\n",
      "Nearest to a: the, dasyprocta, agouti, galt,\n",
      "Nearest to for: and, in, of, with,\n",
      "Nearest to system: autocratic, ep, exceptions, norm,\n",
      "Nearest to its: the, his, their, agouti,\n",
      "Nearest to these: many, the, undertaken, amphibians,\n",
      "Nearest to up: steward, defamation, dasyprocta, agave,\n",
      "Nearest to see: is, and, zero, tubing,\n",
      "Nearest to six: nine, eight, zero, five,\n",
      "Nearest to while: truetype, hakama, cartilage, soap,\n",
      "Nearest to are: were, is, was, boroughs,\n",
      "\n",
      "Average loss at step  22000 :  7.04287395906\n",
      "Average loss at step  24000 :  6.85738616741\n",
      "Average loss at step  26000 :  6.63193470681\n",
      "Average loss at step  28000 :  6.50598870647\n",
      "Average loss at step  30000 :  5.93556881201\n",
      "Nearest to may: would, can, smalltalk, dasyprocta,\n",
      "Nearest to years: architecture, luthor, cl, two,\n",
      "Nearest to was: is, were, had, by,\n",
      "Nearest to UNK: dasyprocta, trinomial, agouti, circ,\n",
      "Nearest to or: and, dasyprocta, circ, agouti,\n",
      "Nearest to as: dasyprocta, primigenius, touchdown, by,\n",
      "Nearest to a: the, dasyprocta, gemological, agouti,\n",
      "Nearest to for: in, and, of, with,\n",
      "Nearest to system: autocratic, exceptions, remake, archie,\n",
      "Nearest to its: the, his, their, agouti,\n",
      "Nearest to these: many, the, reuptake, some,\n",
      "Nearest to up: steward, defamation, dasyprocta, nutation,\n",
      "Nearest to see: is, tubing, rapper, jon,\n",
      "Nearest to six: eight, nine, five, zero,\n",
      "Nearest to while: abet, truetype, although, jura,\n",
      "Nearest to are: were, is, was, boroughs,\n",
      "\n",
      "Average loss at step  32000 :  5.9841864481\n",
      "Average loss at step  34000 :  5.69492228913\n",
      "Average loss at step  36000 :  5.77074874723\n",
      "Average loss at step  38000 :  5.48801390314\n",
      "Average loss at step  40000 :  5.22935062134\n",
      "Nearest to may: would, can, will, should,\n",
      "Nearest to years: architecture, luthor, cl, agouti,\n",
      "Nearest to was: is, were, had, by,\n",
      "Nearest to UNK: dasyprocta, four, three, agouti,\n",
      "Nearest to or: and, dasyprocta, circ, zero,\n",
      "Nearest to as: dasyprocta, touchdown, primigenius, agave,\n",
      "Nearest to a: the, no, agouti, dasyprocta,\n",
      "Nearest to for: in, of, primigenius, from,\n",
      "Nearest to system: lookups, remake, exceptions, archie,\n",
      "Nearest to its: their, his, the, agouti,\n",
      "Nearest to these: many, some, the, reuptake,\n",
      "Nearest to up: steward, defamation, nutation, automobiles,\n",
      "Nearest to see: zero, tubing, ola, is,\n",
      "Nearest to six: eight, seven, five, four,\n",
      "Nearest to while: although, truetype, jura, abet,\n",
      "Nearest to are: were, is, boroughs, was,\n",
      "\n",
      "Average loss at step  42000 :  5.35635885751\n",
      "Average loss at step  44000 :  5.25108688116\n",
      "Average loss at step  46000 :  5.21024729431\n",
      "Average loss at step  48000 :  5.23680193329\n",
      "Average loss at step  50000 :  5.02199324381\n",
      "Nearest to may: would, can, will, should,\n",
      "Nearest to years: architecture, luthor, cl, four,\n",
      "Nearest to was: is, were, had, has,\n",
      "Nearest to UNK: kapoor, dasyprocta, agouti, circ,\n",
      "Nearest to or: and, kapoor, five, dasyprocta,\n",
      "Nearest to as: dasyprocta, primigenius, kifl, touchdown,\n",
      "Nearest to a: the, gemological, agouti, circ,\n",
      "Nearest to for: in, primigenius, kapoor, of,\n",
      "Nearest to system: lookups, pressurised, exceptions, schuschnigg,\n",
      "Nearest to its: their, his, the, agouti,\n",
      "Nearest to these: many, some, reuptake, both,\n",
      "Nearest to up: steward, kapoor, defamation, automobiles,\n",
      "Nearest to see: ola, tubing, franchisee, bobble,\n",
      "Nearest to six: eight, five, four, seven,\n",
      "Nearest to while: and, although, truetype, abet,\n",
      "Nearest to are: were, is, have, was,\n",
      "\n",
      "Average loss at step  52000 :  5.028330966\n",
      "Average loss at step  54000 :  5.16011765242\n",
      "Average loss at step  56000 :  5.05418796337\n",
      "Average loss at step  58000 :  5.03540117955\n",
      "Average loss at step  60000 :  4.97255556941\n",
      "Nearest to may: would, can, should, will,\n",
      "Nearest to years: architecture, four, cl, luthor,\n",
      "Nearest to was: is, were, had, has,\n",
      "Nearest to UNK: kapoor, dasyprocta, michelob, agouti,\n",
      "Nearest to or: and, kapoor, dasyprocta, michelob,\n",
      "Nearest to as: dasyprocta, michelob, hominem, primigenius,\n",
      "Nearest to a: the, pulau, gemological, agouti,\n",
      "Nearest to for: of, primigenius, in, kapoor,\n",
      "Nearest to system: lookups, pressurised, remake, autocratic,\n",
      "Nearest to its: their, his, the, agouti,\n",
      "Nearest to these: many, some, both, three,\n",
      "Nearest to up: steward, biomes, kapoor, defamation,\n",
      "Nearest to see: pulau, franchisee, ola, bobble,\n",
      "Nearest to six: eight, five, four, seven,\n",
      "Nearest to while: although, and, but, where,\n",
      "Nearest to are: were, is, have, be,\n",
      "\n",
      "Average loss at step  62000 :  5.01934396243\n",
      "Average loss at step  64000 :  4.83145305419\n",
      "Average loss at step  66000 :  4.60456622338\n",
      "Average loss at step  68000 :  4.98372616303\n",
      "Average loss at step  70000 :  4.90416188312\n",
      "Nearest to may: would, can, will, could,\n",
      "Nearest to years: architecture, cardiomyopathy, cl, four,\n",
      "Nearest to was: is, were, had, has,\n",
      "Nearest to UNK: kapoor, ursus, dasyprocta, trinomial,\n",
      "Nearest to or: and, kapoor, cardiomyopathy, michelob,\n",
      "Nearest to as: leontopithecus, michelob, dasyprocta, ursus,\n",
      "Nearest to a: the, pulau, any, dasyprocta,\n",
      "Nearest to for: primigenius, of, kapoor, michelob,\n",
      "Nearest to system: lookups, dinar, aba, pressurised,\n",
      "Nearest to its: their, his, the, agouti,\n",
      "Nearest to these: many, some, they, such,\n",
      "Nearest to up: steward, microcebus, kapoor, biomes,\n",
      "Nearest to see: pulau, UNK, franchisee, tubing,\n",
      "Nearest to six: eight, five, four, seven,\n",
      "Nearest to while: although, but, and, where,\n",
      "Nearest to are: were, is, have, cebus,\n",
      "\n",
      "Average loss at step  72000 :  4.74963695586\n",
      "Average loss at step  74000 :  4.82034003484\n",
      "Average loss at step  76000 :  4.72504755509\n",
      "Average loss at step  78000 :  4.79898071313\n",
      "Average loss at step  80000 :  4.7972455287\n",
      "Nearest to may: can, would, will, could,\n",
      "Nearest to years: architecture, cardiomyopathy, cl, dinar,\n",
      "Nearest to was: is, had, were, has,\n",
      "Nearest to UNK: kapoor, ursus, microcebus, circ,\n",
      "Nearest to or: and, kapoor, cardiomyopathy, busan,\n",
      "Nearest to as: michelob, leontopithecus, dasyprocta, hominem,\n",
      "Nearest to a: the, any, dasyprocta, agouti,\n",
      "Nearest to for: of, michelob, or, kapoor,\n",
      "Nearest to system: lookups, aba, dinar, schuschnigg,\n",
      "Nearest to its: their, his, the, thaler,\n",
      "Nearest to these: many, some, such, those,\n",
      "Nearest to up: steward, off, defamation, microcebus,\n",
      "Nearest to see: olden, franchisee, pulau, kapoor,\n",
      "Nearest to six: eight, five, four, seven,\n",
      "Nearest to while: although, but, and, where,\n",
      "Nearest to are: were, is, have, dasyprocta,\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  82000 :  4.77486974752\n",
      "Average loss at step  84000 :  4.74932595718\n",
      "Average loss at step  86000 :  4.77752174807\n",
      "Average loss at step  88000 :  4.74899364591\n",
      "Average loss at step  90000 :  4.73518619788\n",
      "Nearest to may: can, would, will, could,\n",
      "Nearest to years: architecture, cardiomyopathy, cl, luthor,\n",
      "Nearest to was: is, had, were, has,\n",
      "Nearest to UNK: kapoor, microcebus, dasyprocta, ursus,\n",
      "Nearest to or: and, kapoor, cardiomyopathy, michelob,\n",
      "Nearest to as: hominem, leontopithecus, michelob, dasyprocta,\n",
      "Nearest to a: the, any, pulau, agouti,\n",
      "Nearest to for: primigenius, michelob, busan, kapoor,\n",
      "Nearest to system: dinar, aba, lookups, schuschnigg,\n",
      "Nearest to its: their, his, the, thaler,\n",
      "Nearest to these: many, some, such, both,\n",
      "Nearest to up: steward, off, microcebus, them,\n",
      "Nearest to see: olden, franchisee, pulau, called,\n",
      "Nearest to six: eight, seven, five, four,\n",
      "Nearest to while: although, but, and, escuela,\n",
      "Nearest to are: were, is, have, be,\n",
      "\n",
      "Average loss at step  92000 :  4.66819065404\n",
      "Average loss at step  94000 :  4.71604165459\n",
      "Average loss at step  96000 :  4.6880214107\n",
      "Average loss at step  98000 :  4.59317743933\n",
      "Average loss at step  100000 :  4.68962620378\n",
      "Nearest to may: can, would, will, could,\n",
      "Nearest to years: architecture, cl, cardiomyopathy, luthor,\n",
      "Nearest to was: is, had, were, has,\n",
      "Nearest to UNK: kapoor, circ, dasyprocta, trinomial,\n",
      "Nearest to or: and, kapoor, busan, michelob,\n",
      "Nearest to as: michelob, leontopithecus, dasyprocta, hominem,\n",
      "Nearest to a: the, any, pulau, mtsho,\n",
      "Nearest to for: in, kapoor, michelob, busan,\n",
      "Nearest to system: aba, dinar, lookups, thaler,\n",
      "Nearest to its: their, his, the, thaler,\n",
      "Nearest to these: many, some, such, those,\n",
      "Nearest to up: off, steward, out, them,\n",
      "Nearest to see: franchisee, olden, kapoor, pulau,\n",
      "Nearest to six: seven, eight, five, three,\n",
      "Nearest to while: although, but, when, and,\n",
      "Nearest to are: were, is, have, dasyprocta,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Begin training.\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch( batch_size, num_skips)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # estimated loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange( valid_size):\n",
    "                valid_word = rev_embed[valid_examples[i]]\n",
    "                top_k = 4    # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = rev_embed[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "            print(\"\")\n",
    "                \n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Visualize the embeddings.\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(18, 18))    # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "# pylint: disable=g-import-not-at-top\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "plot_only = 500\n",
    "low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "labels = [rev_embed[i] for i in xrange(plot_only)]\n",
    "plot_with_labels(low_dim_embs, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a a a a a a a a a a a a a a a a "
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 16 is out of bounds for axis 0 with size 16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0df390f616f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev_embed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mnearest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_k\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#     word = random.choice(nearest)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 16 is out of bounds for axis 0 with size 16"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
